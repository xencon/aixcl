# Plan: Report Configured Models in Stack and Service Output

**Goal:** Show configured models for Ollama, Council, and Continue as part of stack status and the respective service output. When no models are configured, report that explicitly.

**Scope:** `./aixcl stack status`, `./aixcl council status` (align messaging), and any place that shows "service output" for Ollama, Council, or Continue (today that is only stack status; no separate `service status` command).

---

## 1. Definitions

| Service / component | "Configured models" source | No models / unavailable |
|--------------------|----------------------------|---------------------------|
| **Ollama** | **Current default model only.** Ollama can have a default model set via the `OLLAMA_MODEL` environment variable in the container. If unset, Ollama has no single default (user/app chooses per request). | Container not running → "(Ollama not running)"; running but `OLLAMA_MODEL` unset → "Default: (not set)". |
| **Council** | Chairman + council members from `.env` (`CHAIRMAN_MODEL`, `COUNCIL_MODELS`). | Both unset or empty → "No models configured". |
| **Continue (VS Code plugin)** | Models defined in the **VS Code plugin config**: `.continue/council/config.yaml` in the repo. Parse `models[].name` or `models[].model`. | File missing or no models block → "(no config found)" or "(no models in config)". |
| **Continue CLI** | Config and status for the **Continue CLI** (`cn`): config file `.continue/cli-ollama.yaml` (generated by `./aixcl continue config`). Show config path and model list (or count) from that file. | File missing → "(config not generated)" or "Run: ./aixcl continue config". |

**Decisions:**
- **Ollama:** Show only the **current default model** (from `OLLAMA_MODEL` in the Ollama container). If unset, show "(not set)".
- **Council:** Unchanged: chairman + members from `.env`.
- **Continue (VS Code plugin):** Reference **only the VS Code plugin config** (`.continue/council/config.yaml`). Report model names from that file.
- **Continue CLI:** Add a **separate status line** for the Continue CLI: config path, and models from `.continue/cli-ollama.yaml` (or "(config not generated)").

---

## 2. Where to display

### 2.1 Stack status (`./aixcl stack status`)

- **Placement:** After the "Runtime Core" block (after the three lines for Ollama, Council, continue), add a **"Configured Models"** subsection before "Operational Services".
- **Content:**
  - **Ollama:** One line: `Default: <model name>` from `OLLAMA_MODEL` in container, or `Default: (not set)` if unset. If Ollama container is not running: `Default: (Ollama not running)`.
  - **Council:** One line: `Chairman: <name>; Members: <list or "none">`. If .env has no chairman/council: `Chairman: (not set); Members: (not set)`.
  - **Continue (VS Code plugin):** One line: `Models: <names from .continue/council/config.yaml>`. If file missing: `Models: (no config found)`.
  - **Continue CLI:** One line: `Config: .continue/cli-ollama.yaml; Models: <list from that file>` or `Config: (not generated). Run: ./aixcl continue config`. This gives status for the Continue CLI separately from the VS Code plugin.
- **Empty/unset:** Always show something explicit (e.g. "(not set)", "(no config found)").

### 2.2 Council status (`./aixcl council status`)

- Already shows "Chairman: ..." and "Council Members: ..." with `<not set>` when empty, and "Council is not configured yet" when both are empty.
- **Alignment:** Ensure the wording is consistent with stack status (e.g. "No models configured" in one place and "(not set)" in the other is fine; avoid contradicting).
- Optional: Add one line at the top like "Configured models: chairman + N members" or leave as-is and only change stack status.

### 2.3 Service output

- There is no separate `./aixcl service status <name>` command. "Service output for the respective services" is interpreted as the **stack status lines and the new "Configured Models" block** for Ollama, Council, and Continue. No new subcommand required.

---

## 3. Data sources and helpers

### 3.1 Ollama (default model only)

- **Source:** Default model is set by the `OLLAMA_MODEL` environment variable in the Ollama container. Get it with: `docker exec <ollama_container> env | grep OLLAMA_MODEL` (or print the value). If the variable is not set, Ollama has no single default.
- **Helper:** `get_ollama_container` then `docker exec "$container" env 2>/dev/null | grep '^OLLAMA_MODEL=' | cut -d= -f2-`. If container not running → "(Ollama not running)". If `OLLAMA_MODEL` is unset or empty → "Default: (not set)".
- **Format:** One line: `Ollama:   Default: <model>` or `Ollama:   Default: (not set)` or `Ollama:   Default: (Ollama not running)`.

### 3.2 Council

- **Source:** Same .env read as in `status_council`: `CHAIRMAN_MODEL`, `COUNCIL_MODELS` (comma-separated). Read from `SCRIPT_DIR/.env`.
- **Helper:** Either a small function in `aixcl` (e.g. `get_council_configured_models`) that echoes chairman and members, or inline read of .env in the status block. Prefer a small helper for clarity.
- **Format:** `Chairman: X; Members: A, B, C` or `Chairman: (not set); Members: (not set)`.

### 3.3 Continue (VS Code plugin)

- **Source:** `.continue/council/config.yaml` in repo (SCRIPT_DIR). Parse YAML for `models:` and then each entry’s `name:` or `model:`.
- **Parsing in bash:** Use grep/sed/awk to extract lines (e.g. `grep -E '^\s+(name|model):' .continue/council/config.yaml | sed 's/.*:\s*//' | tr -d ' '`).
- **Fallback:** If file does not exist, show "Models: (no config found)". If file exists but no models block → "Models: (no models in config)".
- **Format:** One line: `Continue (VS Code):  Models: name1, name2, ...` or `(no config found)`.

### 3.4 Continue CLI

- **Source:** `.continue/cli-ollama.yaml` (generated by `./aixcl continue config`). Same parsing for `model:` or `name:` under `models:`.
- **Helper:** If file missing → "Config: (not generated). Run: ./aixcl continue config". If present, list model names and show config path.
- **Format:** One line: `Continue CLI:  Config: .continue/cli-ollama.yaml; Models: a, b, c` or `Continue CLI:  Config: (not generated). Run: ./aixcl continue config`.

---

## 4. Implementation outline

### 4.1 aixcl script

1. **Helper: `report_ollama_default_model`**  
   If `get_ollama_container` is empty → "Ollama:   Default: (Ollama not running)". Else read `OLLAMA_MODEL` from container env (`docker exec ... env | grep OLLAMA_MODEL`); if unset or empty → "Ollama:   Default: (not set)"; else "Ollama:   Default: <value>".

2. **Helper: `report_council_models`**  
   Read CHAIRMAN_MODEL and COUNCIL_MODELS from `.env` (same logic as status_council). Output "Council:  Chairman: X; Members: A, B, C" or "Council:  Chairman: (not set); Members: (not set)".

3. **Helper: `report_continue_plugin_models`**  
   If `[ -f .continue/council/config.yaml ]` then parse model names (grep/sed for `name:` or `model:` under models); else "Continue (VS Code):  (no config found)". If file exists but no models → "Continue (VS Code):  (no models in config)".

4. **Helper: `report_continue_cli_status`**  
   If `[ -f .continue/cli-ollama.yaml ]` then parse model names and output "Continue CLI:  Config: .continue/cli-ollama.yaml; Models: a, b, c". Else "Continue CLI:  Config: (not generated). Run: ./aixcl continue config".

5. **In `status()` (stack status):**  
   After the Runtime Core block (after the "continue" line) and before "Operational Services", add:
   - Section header: "Configured Models".
   - Call the four helpers and print their output (one line each: Ollama default, Council, Continue VS Code plugin, Continue CLI).

6. **Optional:** In `status_council()`, keep existing "No models configured" / "<not set>" messaging consistent with stack status.

### 4.2 Docs

- **`docs/architecture/governance/03_stack_status.md`:** Extend the "Example Default Output" to include the new "Configured Models" block and state that Ollama/Council/Continue show configured models (or explicit "none" / "not set" / "not running").

### 4.3 Edge cases

- **Ollama:** Container running but `docker exec ... env` fails → show "Default: (unable to read)". If `OLLAMA_MODEL` is set to a model that is not installed, still show the value (no validation).
- **Council:** .env missing → same as today (e.g. "Council: (not set)" or "(.env not found)").
- **Continue (VS Code):** Only `.continue/council/config.yaml` is used for the plugin line.
- **Continue CLI:** Only `.continue/cli-ollama.yaml` is used; if missing, direct user to run `./aixcl continue config`.

---

## 5. Example stack status output (after change)

```
AIXCL Stack Status
==================

Profile: dev
Status: Running

Runtime Core (Strict - Always Enabled)
---------------------------------------
  ✅ Ollama
  ✅ Council
  ✅ continue        Active     Connected (VS Code plugin)

Configured Models
-----------------
  Ollama:             Default: deepseek-coder:1.3b
  Council:            Chairman: deepseek-coder:1.3b; Members: codegemma:2b, qwen2.5-coder:3b
  Continue (VS Code): Models: Council (Multi-Model), Autodetect, Nomic Embeddings
  Continue CLI:       Config: .continue/cli-ollama.yaml; Models: Qwen3 Coder, Glm 4.7 Flash, ...

Operational Services (Guided - Profile-Dependent)
--------------------------------------------------
  ...
```

When nothing or partial:

```
Configured Models
-----------------
  Ollama:             Default: (not set)
  Council:            Chairman: (not set); Members: (not set)
  Continue (VS Code): (no config found)
  Continue CLI:       Config: (not generated). Run: ./aixcl continue config
```

---

## 6. Checklist (for implementation)

- [ ] Add `report_ollama_default_model` (OLLAMA_MODEL from container env; default only).
- [ ] Add `report_council_models` (read .env CHAIRMAN_MODEL, COUNCIL_MODELS).
- [ ] Add `report_continue_plugin_models` (parse .continue/council/config.yaml — VS Code plugin only).
- [ ] Add `report_continue_cli_status` (parse .continue/cli-ollama.yaml; config path + models or "not generated").
- [ ] In `status()`, insert "Configured Models" block after Runtime Core (four lines: Ollama, Council, Continue VS Code, Continue CLI).
- [ ] Ensure "not set" / "not running" / "no config" are explicit for each.
- [ ] Update `03_stack_status.md` example and any AI guidance.
- [ ] Optional: Small consistency pass in `status_council()` wording.

---

*Plan for stack/service output showing configured models for Ollama, Council, and Continue.*
